// src/app/api/agents/executor-text/route.ts
import { NextResponse } from 'next/server';
import type { NextRequest } from 'next/server';
import { ai as genkitAi } from '@/ai/genkit'; // Using genkit for LLM
import type { UserContext } from '@/ai/flows/interpret-user-intent-flow';

interface TextExecutorInput {
  promptFragment: string; // Specific part of the prompt for text generation/synthesis
  originalPrompt: string;
  contextData?: any; // Optional additional context from previous tasks
  userContext?: UserContext; // Added userContext
  hasImageContext?: boolean; // Added for context
}

async function synthesizeText(input: TextExecutorInput): Promise<any> {
  console.log(`[ExecutorText] Synthesizing text for: "${input.promptFragment}"`);
  if (input.userContext) {
    console.log(`[ExecutorText] User context (tone: ${input.userContext.preferredTone || 'default'}) will be considered.`);
  }

  const useMock = !process.env.GENAI_API_KEY || process.env.USE_MOCK_ORKES_CLIENT === 'true';

  if (useMock) {
    await new Promise(resolve => setTimeout(resolve, 1000 + Math.random() * 1000));
    let mockText = `Mock synthesized text for: "${input.promptFragment}". Original prompt context: "${input.originalPrompt}".`;
    if (input.contextData) mockText += ` Additional context: ${JSON.stringify(input.contextData)}.`;
    if (input.userContext?.preferredTone === 'technical') mockText += ` This response is styled technically.`;
    else if (input.userContext?.preferredTone === 'casual') mockText += ` Hey, so this is styled casually, ya know?`;
    mockText += ` This text is generated by a mock agent.`;
    
    return {
      synthesizedText: mockText,
      status: "COMPLETED_MOCK",
      promptFragment: input.promptFragment, // Echo back for synthesizer
    };
  }

  try {
    let llmSystemPrompt = `You are an AI expert tasked with addressing a specific sub-task derived from a larger user request.
Original User Request: "${input.originalPrompt}"
${input.hasImageContext ? "Image context was also provided for the original request." : ""}
Current Sub-task for you to address: "${input.promptFragment}"
`;

    if (input.contextData) {
      llmSystemPrompt += `\nRelevant Context from other agents or data sources: ${typeof input.contextData === 'string' ? input.contextData : JSON.stringify(input.contextData, null, 2)}\n`;
    }
    if (input.userContext?.preferredTone) {
      llmSystemPrompt += `\nPlease adopt a ${input.userContext.preferredTone} tone in your response.`;
    }

    llmSystemPrompt += `\nProvide a concise and relevant response specifically for the current sub-task: "${input.promptFragment}". Ensure your output directly addresses this sub-task while being mindful of the overall original request.`;

    const response = await genkitAi.generate({
      prompt: llmSystemPrompt,
      model: 'googleai/gemini-2.0-flash', 
    });

    return {
      synthesizedText: response.text,
      status: "COMPLETED_REAL",
      promptFragment: input.promptFragment, // Echo back for synthesizer
    };
  } catch (error: any) {
    console.error(`[ExecutorText] Genkit text generation failed: ${error.message}`);
    return {
      error: `Failed to synthesize text: ${error.message}`,
      status: "FAILED",
      promptFragment: input.promptFragment,
    };
  }
}

export async function POST(request: NextRequest) {
  try {
    const body = await request.json();
    const input: TextExecutorInput = body.input || body;

    if (!input || !input.promptFragment) {
      return NextResponse.json({ error: "Missing or invalid input for text executor" }, { status: 400 });
    }

    console.log("[ExecutorText] Received input:", JSON.stringify(input).substring(0,300)+"...");
    
    const result = await synthesizeText(input);
    
    console.log("[ExecutorText] Text synthesis result status:", result.status);
    return NextResponse.json({ ...result });

  } catch (error: any) {
    console.error("[ExecutorText] Error:", error);
    return NextResponse.json({ error: error.message || "An unknown error occurred" }, { status: 500 });
  }
}
